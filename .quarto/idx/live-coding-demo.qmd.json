{"title":"Location Extraction from News Articles, using R and Python","markdown":{"yaml":{"title":"Location Extraction from News Articles, using R and Python","author":"Simona Bisiani","format":"html","execute":{"eval":false,"echo":true},"editor":"visual"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nWelcome to this live coding demonstration exploring **LLMs for Toponym Disambiguation** in UK local media. We will be using [LMUK-Geo](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SGVXIU), a benchmark dataset we annotated for the purpose of testing geoparsing methods on British local news articles. We'll do the following:\n\n-   toponym recognition using *spaCy* in Python, to extract locations\n\n-   candidate retrieval, to find real world candidates, using *OpenStreetMap Nominatim API*\n\n-   toponym resolution with *Ollama* and gemma2-9b model, to disambiguate\n\nAs such, you'll get an overview of the full geoparsing pipeline, and get a sense of popular frameworks and common challenges.\n\n#### Setup: Loading Libraries\n\nLet's start by loading our libraries:\n\n```{r setup}\n#| eval: false\n#| message: false\n#| warning: false\n\n# Load R libraries\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(reticulate)  # For Python integration\n  library(httr2)       # For API calls\n  library(jsonlite)    # For JSON handling\n  library(glue)        # For string templating\n  library(DT)          # For interactive tables\n  library(maps)\n})\n\n# use_virtualenv(\".venv/\")\nuse_virtualenv('.venv-mac/')\n# py_install(\"pandas\")\n# py_install('spacy[transformers]')\n# system(\"python -m spacy download en_core_web_sm\")\n```\n\n```{python libraries}\nimport pandas as pd\nimport numpy as np\nimport ast\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n------------------------------------------------------------------------\n\n## Dataset\n\nLet's examine our benchmark dataset:\n\n```{r gold-data-overview}\n#| message: false\n#| warning: false\n#| fig-cap: \"Distribution of locations inside the LMUK-Geo dataset\"\n#| column: margin\n\ngold_data = read_csv('data/LMUK-Geo.csv')\ngold_data |> glimpse()\n# gold_data |> select(doc) |> n_distinct()\n# gold_data |> select(domain) |> n_distinct()\n\nggplot() + \n  geom_polygon(data = map_data('world'), \n  aes(x = long, y = lat, group = group), \n  fill = 'gray90', \n  color = 'black') + \n  coord_fixed(xlim = c(-10,3), \n              ylim = c(50.3, 59))+\n  geom_point(data = gold_data, \n  aes(x = Longitude, y = Latitude, color = domain), \n  size = 2.5, alpha = 0.2) +\n  theme_minimal()+\n  theme(legend.position = \"none\")\n```\n\n------------------------------------------------------------------------\n\n## Toponym Recognition using spaCy\n\nRemember toponym recognition? The idea that, given some text, we extract the locations inside of it. Toponym recognition is a subset of Named Entity Recognition (NER), and, like NER, it's done in 3 ways: rules, learning, or hybrid. *Rules* consists of matching some patterns in the text. For example, anything matching the pattern 'City of' and the next word, or a word followed by 'Street', or 'Boulevard'.\n\n```{r NER-rule}\n# Sample text with locations\ntext <- \"I traveled from the City of Boston to New York, walking down Main Street and Broadway Boulevard. Then I visited Paris, France and returned to Los Angeles County.\"\n\n# Define simple patterns for location recognition\npatterns <- c(\n  \"City of \\\\w+\",           # City of [name]\n  \"\\\\w+ Street\",            # [name] Street  \n  \"\\\\w+ Boulevard\",         # [name] Boulevard\n  \"\\\\w+ County\",            # [name] County\n  \"\\\\w+, \\\\w+\"             # [City], [State/Country]\n)\n\n# Extract locations using pattern matching\nlocations <- c()\nfor(pattern in patterns) {\n  matches <- regmatches(text, gregexpr(pattern, text, perl = TRUE))[[1]]\n  locations <- c(locations, matches)\n}\n\n# Remove duplicates and display results\nunique_locations <- unique(locations)\nprint(unique_locations)\n```\n\nThe issue with rules is that to capture all instances you need a lot of them. The more rules you have the more false positives you will also have. Luckily, *learning-based* systems do a pretty good job nowadays. Sometimes researchers find it helpful to mix rules and learning-based methods, and thus this category is called *hybrid*. Learning-based NER trains machine learning models on large annotated datasets where humans have labeled entities like persons, locations, and organizations, allowing the system to automatically recognize similar patterns in new text without hand-crafted rules. [spaCy](https://spacy.io/usage) NER is a popular Python library that provides pre-trained neural network models for named entity recognition, offering out-of-the-box entity extraction in multiple languages with high accuracy and the ability to customize models for specific domains.\n\nThere are multiple models offered by spaCy, and which to choose mostly depends on where on the efficiency/accuracy spectrum you would like to be. In other words, how fast vs how correct. Sometimes, if your machine is not powerful enough, your choice might be driven by model size (guess: smaller = worse). You can check performances of various models by spaCy [on their website](images/clipboard-303220680.png)\\](https://spacy.io/usage/facts-figures)\n\n```{python spacy-NER-test}\nimport spacy\nfrom spacy import displacy\n\narticles = pd.read_csv('data/articles.csv')\nlen(articles)\narticles.head()\n\n# measure time\nnlp = spacy.load(\"en_core_web_trf\")  # Load the spaCy transformer model\n\nstart = pd.Timestamp.now()\ndoc = nlp(articles['article'][1])\nend = pd.Timestamp.now()\nprint(\"Time taken:\", end - start)\n\nentities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE']\nhtml = displacy.render(doc, style=\"ent\")\n\nnlp = spacy.load(\"en_core_web_sm\")  # Load the spaCy small model\n\nstart = pd.Timestamp.now()\ndoc = nlp(articles['article'][1])\nend = pd.Timestamp.now()\nprint(\"Time taken:\", end - start)\n\nentities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE']\nhtml1 = displacy.render(doc, style=\"ent\")\n```\n\nWe can display the results to get an intuitive understanding of what the NER model does.\n\n```{r}\n# Display the HTML from Python\nhtmltools::HTML(py$html)\nhtmltools::HTML(py$html1)\n```\n\nLet's now run these models on all our articles, and benchmark their performance against the gold annotation. The models look for three types of location entities:**GPE**: Geopolitical entities (countries, cities, states); **LOC**: Locations (mountains, water bodies, regions); **FAC**: Facilities (buildings, airports, highways). Each model's predictions are compred against the human-annotated *gold standard* using standard information retrieval metrics. We will define:\n\n-   **True Positives**: Geographic entities correctly identified by the model\\\n-   **False Positives**: Non-geographic terms incorrectly identified as locations\\\n-   **False Negatives**: Geographic entities that the model missed\\\n\nand use these to calculate:\n\n**Precision** = TP / (TP + FP)\\\n\\> *Of all the entities the model identified as locations, what percentage were actually correct?*\\\nHigh precision = low false alarm rate\n\n**Recall** = TP / (TP + FN)\\\n\\> *Of all the actual geographic entities in the text, what percentage did the model successfully find?*\\\nHigh recall = low miss rate\n\n**F1 Score** = 2 × (Precision × Recall) / (Precision + Recall)\\\n\\> Harmonic mean that balances precision and recall.\\\n\\> Useful single metric when you need both high precision AND high recall.\n\nWe will then produce a **comparison table** showing precision, recall, and F1 scores for both models, helping determine which performs better for toponym recognition tasks.\n\n```{python}\n# Load human-annotated gold standard data\ngold_df = pd.read_csv('data/gold_toponym_recognition.csv')\n\n# ------------------------------------------------------------\n# STEP 1: Parse the gold standard annotations\n# ------------------------------------------------------------\n# The entities_gold column contains string representations of dicts\n# We need to convert them to actual Python dictionaries\n\ndef safe_literal_eval(x):\n    \"\"\"Safely convert string to dictionary\"\"\"\n    if isinstance(x, dict):\n        return x\n    try:\n        return ast.literal_eval(x)\n    except (ValueError, SyntaxError):\n        return {}\n\ngold_df['entities_gold'] = gold_df['entities_gold'].apply(safe_literal_eval)\n\n# ------------------------------------------------------------\n# STEP 2: Extract entities using both spaCy models\n# ------------------------------------------------------------\nnlp_trf = spacy.load(\"en_core_web_trf\")  # Transformer (accurate but slow)\nnlp_sm = spacy.load(\"en_core_web_sm\")    # Small model (fast but less accurate)\n\n# We care about these location entity types:\n# GPE = Geopolitical entities (countries, cities, states)\n# LOC = Locations (mountains, water bodies, regions)  \n# FAC = Facilities (buildings, airports, highways)\nVALID_LABELS = {\"LOC\", \"GPE\", \"FAC\"}\n\ndef extract_entities(text, nlp_model):\n    \"\"\"Extract location entities from text using spaCy\"\"\"\n    doc = nlp_model(text)\n    return {\n        label: [ent.text.strip() for ent in doc.ents if ent.label_ == label]\n        for label in VALID_LABELS\n    }\n\n# Process all documents with both models\ntrf_results = []\nsm_results = []\n\nfor doc_text in tqdm(gold_df['doc'], desc=\"Processing documents\"):\n    trf_results.append(extract_entities(doc_text, nlp_trf))\n    sm_results.append(extract_entities(doc_text, nlp_sm))\n\n# Add model predictions to dataframes\ngold_df['entities_trf'] = trf_results\ngold_df['entities_sm'] = sm_results\n# gold_df.to_csv('data/gold_with_predictions.csv', index=False)\n# ------------------------------------------------------------\n# STEP 3: Calculate performance metrics\n# ------------------------------------------------------------\n\ndef evaluate_model(gold_entities, predicted_entities):\n    \"\"\"\n    Compare predicted entities against gold standard.\n    Returns precision, recall, and F1 scores.\n    \"\"\"\n    # Flatten all entity types into single lists and lowercase for fair comparison\n    gold_flat = []\n    pred_flat = []\n    \n    for entity_type in VALID_LABELS:\n        gold_flat.extend([e.lower() for e in gold_entities.get(entity_type, [])])\n        pred_flat.extend([e.lower() for e in predicted_entities.get(entity_type, [])])\n    \n    # Convert to sets for comparison\n    gold_set = set(gold_flat)\n    pred_set = set(pred_flat)\n    \n    # Calculate true positives, false positives, false negatives\n    tp = len(gold_set & pred_set)  # Correctly found\n    fp = len(pred_set - gold_set)  # Incorrectly found\n    fn = len(gold_set - pred_set)  # Missed\n    \n    return tp, fp, fn\n\n# Calculate metrics for each document\ntrf_metrics = []\nsm_metrics = []\n\nfor idx, row in gold_df.iterrows():\n    # Transformer model\n    tp, fp, fn = evaluate_model(row['entities_gold'], row['entities_trf'])\n    trf_metrics.append({'TP': tp, 'FP': fp, 'FN': fn})\n    \n    # Small model\n    tp, fp, fn = evaluate_model(row['entities_gold'], row['entities_sm'])\n    sm_metrics.append({'TP': tp, 'FP': fp, 'FN': fn})\n\n# ------------------------------------------------------------\n# STEP 4: Calculate overall performance \n# ------------------------------------------------------------\n\ndef calculate_micro_averaged_scores(metrics_list):\n    \"\"\"Calculate overall precision, recall, and F1 across all documents\"\"\"\n    total_tp = sum(m['TP'] for m in metrics_list)\n    total_fp = sum(m['FP'] for m in metrics_list)\n    total_fn = sum(m['FN'] for m in metrics_list)\n    \n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1\n\n# Get final scores\ntrf_precision, trf_recall, trf_f1 = calculate_micro_averaged_scores(trf_metrics)\nsm_precision, sm_recall, sm_f1 = calculate_micro_averaged_scores(sm_metrics)\n\nresults = pd.DataFrame({\n    \"Model\": [\"en_core_web_trf\", \"en_core_web_sm\"],\n    \"Precision\": [trf_precision, sm_precision],\n    \"Recall\": [trf_recall, sm_recall],\n    \"F1\": [trf_f1, sm_f1]\n})\n\nresults.round(3)\n```\n\nAs we can see, the transformer model is highly superior (althought slower) to the small model. Just so you know, these represent the extremes of the spaCy spectrum and there are other models you could try.  Let's now save the toponyms extracted by our best model, to use in the next sections.\n\n```{python}\nlong_data = []\nfor idx, row in gold_df.iterrows():\n    for entity_type, entities in row['entities_trf'].items():\n        for entity in entities:\n            long_data.append({'doc_id': idx, 'doc': row['doc'], 'entity': entity})\n\npd.DataFrame(long_data).to_csv('data/entities_long.csv', index=False)\n```\n\n------------------------------------------------------------------------\n\n## Candidate Retrieval\n\nFor candidate retrieval, we will use [OpenStreetMap's Nominatim](https://nominatim.org/), an API service that fetches data from OSM's geographic knowledge database. We will use the tidygeocoder package in R, which provides a simple and consistent interface for geocoding and reverse geocoding using various services, including OSM Nominatim. It allows you to convert addresses or place names into geographic coordinates (latitude and longitude) and vice versa.\n\nJust so you know, another useful resource (for UK locations) is Ordnance Survey Open Names, a comprehensive database of place names, roads numbers and postcodes for Great Britain (but no Northern Ireland unfortunately): <https://www.ordnancesurvey.co.uk/products/os-open-names>. Downloading OS Open Names is very simple. From the platform you just click on your desired format and get the data for free. It's updated quarterly.\n\n    ![](images/clipboard-3609027799.png){width=\"358\"}\n\n```{r}\n# Load data\ngold_df <- read_csv('data/entities_long.csv')\nunique_entities <- gold_df |>\n  select(entity) |>\n  distinct()\n# write_csv(unique_entities, 'data/unique_entities.csv')\n\n# OSM candidates (this will take around 8minutes for 470 entities)\nlibrary(tidygeocoder)\n# unique_entities <- read_csv('data/unique_entities.csv')\nosm_candidates <- unique_entities |> \n  geocode(entity, method = 'osm', lat = latitude , long = longitude, limit = 10, custom_query = list(countrycodes = \"gb\"), return_input = FALSE)\nwrite_csv(osm_candidates, 'data/osm_candidates.csv')\n```\n\nBefore we move on to disambiguation let's calculate which district each coordinate falls into. This will help us later on in the disambiguation process.\n\n```{r}\nosm_candidates <- read_csv('data/osm_candidates.csv')\nlibrary(sf)\n# Load UK districts shapefile\nuk_districts <- st_read(\"lad_boundaries/LAD_MAY_2024_UK_BFE.shp\")\nuk_districts <- st_transform(uk_districts, crs = 4326)  # Ensure it's in WGS84\n\n# Convert OSM candidates to sf object\nosm_sf <- st_as_sf(osm_candidates |> drop_na(), coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Perform spatial join to find which district each point falls into\nosm_with_districts <- st_join(osm_sf, uk_districts, join = st_within)\nosm_with_districts_cands <- osm_with_districts |> \n  filter(!is.na(LAD24NM)) |> \n  mutate(\n    longitude = st_coordinates(geometry)[,1],\n    latitude  = st_coordinates(geometry)[,2]\n  )\n\nwrite_csv(osm_with_districts_cands |> st_drop_geometry(), 'data/osm_candidates_with_districts.csv')\n```\n\n------------------------------------------------------------------------\n\n## Toponym Resolution\n\n### Ollama\nTo use ollama in R, you can directly communicate with the local server using the `httr2` package, or you can use the `ellmer`, '[ollamar](https://hauselin.github.io/ollama-r/)' or 'rollama' packages which provide a convenient interface. Regardless, your first need install Ollama on your machine.\n\nIf you choose the 'httr2' route, you can refer to the [Ollama API documentation](https://ollama.com/docs/api) for details on how to structure your requests. I also recommend this [blog post](https://tomsing1.github.io/blog/posts/vectorsearch/).\n\n```{r}\n# this is ellmer works\nchat <- ellmer::chat_ollama(\"Be terse\", model = \"gemma3:1b\", echo = \"none\")\nchat$chat(\"who is Super Mario's best friend?\")\n\n# it's longer code, but more robust over time with httr2\nhttr2::request(\"http://localhost:11434\") |> \n  httr2::req_url_path(\"/api/generate\") |>\n  httr2::req_headers(\"Content-Type\" = \"application/json\") |>\n  httr2::req_body_json(\n    list(\n      model = \"gemma3:1b\",\n      prompt = \"Who is Super Mario's best friend?\",\n      stream = FALSE,\n      options = list(seed = 123)  # reproducible seed\n    )\n  ) |> \n  httr2::req_perform() |> httr2::resp_body_json() |>\n    getElement(\"response\")\n\n# a useful wrapper is ollamar, which hides this code behind a function\nollamar::generate(\"gemma3:1b\", \"Who is Super Mario's best friend?\", output = 'text')\n```\n\n### Experiment Setup\nLet's try setup an experiment where we query gemma3-1b to disambiguate a toponym. We will provide it with the context of the article where the toponym was found, and a list of candidates retrieved from the gazetteers. The model will then select the most likely candidate.\n\n```{r load-data}\n# merge back candidates with articles and entities\ngold_data <- read_csv('data/LMUK-Geo.csv')\nosm_with_districts_cands <- read_csv('data/osm_candidates_with_districts.csv')\n\n# now let's select a small sample of 10 articles to work with\nset.seed(42)\nsample_docs <- gold_data |> \n  distinct(doc) |>            # make sure articles are unique\n  slice_sample(n = 10) |> \n  pull(doc)                   # vector of 10 article texts\n\n# Step 2: keep all toponyms + candidates for those articles\nsample <- osm_with_districts_cands |> \n  sf::st_drop_geometry() |>\n  group_by(address) |>\n  summarise(candidates = str_c(unique(LAD24NM), collapse = \"; \"), .groups = \"drop\") |>\n  left_join(gold_data, by = c(\"address\" = \"text\")) |> \n  filter(doc %in% sample_docs)\n  \n# Define temperatures to test\ntemperatures <- as.character(c(0,1))\ndata_expanded <- expand_grid(\n  temperature = temperatures, \n  sample\n) |> \n  mutate(model = 'gemma3:1b')\n```\n\n\n```{r experiments-setup}\n# Define experiments: which fields to include\nexperiments <- list(\n  exp1 = c(\"Outlet coverage LAD\", \"Domain\"),\n  exp2 = c(\"Domain\")  # Only Domain\n)\n\n# Define prompt types\nprompt <- \"The task is mapping an entity (a toponym) to the Local Authority District (LAD) in which it is situated. Your goal is to select the correct option from the list provided. Instructions:\n1. Review Entity and Article:\n- Identify the toponym (location name).\n- Read the article carefully to understand the context.\nExample:\nEntity: King's Head pub.\nArticle: Incident outside the King's Head pub on Main Street, Guildford.\nUse surrounding text to infer the location (e.g., Guildford).\n\n2. Check Metadata where provided:\n- Domain: The publisher’s domain may provide geographic context.\n- Outlet coverage LAD: The Local Authority District covered by the outlet which published the article.\n- Other Entities: Other entities present in the same articles and their candidates.\n\n3. Select answer from options:\nChoose the correct answer from the options based on context.\nOptions:\n- A list of applicable Districts, if any.\n- “LAD not in options” (choose if correct District is missing).\n- “Entity is not a location” if applicable.\n- “Entity is outside the UK” for non-UK locations.\n- “Entity spans across several districts (e.g., a region)“ for entities that are not specific to a single LAD (e.g., Wales, Sussex).\n- “Unsure” if uncertain.\n\n4. Generate Response:\n- Format your response as JSON:\n{\n  \\'chosen_option\\': \\'Your choice\\',\n  \\'reasoning\\': \\'Your reasoning\\'\n}\"\n\nclassification_question <- \"Which of the options provided best represents the Local Authority District (LAD) for the entity provided, based on the context in the article? Ensure the response is strictly in JSON format with no additional text, explanations, or commentary outside of the JSON object. Match the JSON schema indicated. Example of output: {\\'chosen_option\\': \\'Fife\\', \\'reasoning\\': \\'The article refers to a toponym situated in Fife.\\'}\"\n```\n\n```{r main-function}\nlibrary(jsonlite)\ndetach(package:maps, unload=TRUE)\n\nquery_llm <- function(data, system_message, classification_question, included_fields = c(\"Outlet coverage LAD\", \"Domain\")) {\n  # Create prompts for each row\n  prompts <- data_expanded %>%\n    rowwise() |> \n    mutate(\n      test_prompt = glue(\n        \"{prompt}\\n\",\n        \"Entity: {address}\\n\\n\",\n        \"Article: {doc}\\n\\n\",\n        if (\"Outlet coverage LAD\" %in% included_fields) {\n          \"Outlet coverage LAD: {domain_lad}\\n\"\n        } else {\n          \"\"\n        },\n        if (\"Domain\" %in% included_fields) {\n          \"Domain: {domain}\\n\"\n        } else {\n          \"\"\n        },\n        \"Options: {candidates}\\n\",  # Fix: use candidates instead of options_str\n        \"{classification_question}\"\n      )\n    ) %>%\n    pull(test_prompt)\n  \n  # Create requests\n  reqs <- map2(prompts, data_expanded$temperature, function(prompt, temp) {  \n    httr2::request(\"http://localhost:11434\") %>%\n      httr2::req_url_path(\"/api/generate\") %>%\n      httr2::req_headers(\"Content-Type\" = \"application/json\") %>%\n      httr2::req_body_json(list(\n        model = \"gemma3:1b\", \n        prompt = prompt,\n        stream = FALSE,\n        format = \"json\",\n        keep_alive = \"10s\",\n        options = list(seed = 42, temperature = as.numeric(temp))  # Use temp parameter\n      ))\n  })\n  \n  # Make parallel requests\n  resps <- httr2::req_perform_parallel(reqs, on_error = \"continue\", progress = TRUE)\n  \n  # Process results\n  results <- purrr::map(resps, function(resp) {  # Use purrr::map explicitly\n    # Step 1: parse outer JSON\n    outer <- httr2::resp_body_json(resp)\n    # Step 2: parse inner 'response' JSON safely\n    parsed_inner <- tryCatch({\n      fromJSON(outer$response)\n    }, error = function(e) NULL)\n    tibble(\n      chosen_option = if (!is.null(parsed_inner)) parsed_inner$chosen_option else NA,\n      reasoning     = if (!is.null(parsed_inner)) parsed_inner$reasoning else NA\n    )\n  })\n  \n  # Combine into a single data frame\n  results_df <- bind_rows(results)\n  \n  # Bind with original data to preserve address, doc, and other fields\n  final_result <- bind_cols(\n    data %>% select(address, doc, everything()),  # Keep all original columns\n    results_df\n  )\n  \n  return(final_result)  # Don't forget to return the result!\n}\n\n# Simple iteration through experiments\nfor (exp_name in names(experiments)) {\n  included_fields <- experiments[[exp_name]]\n  \n  # Run the query\n  result <- query_llm(\n    data = data_expanded,\n    system_message = prompt,\n    classification_question = classification_question,\n    included_fields = included_fields\n  )\n  write_csv(result, paste0(\"results_\", exp_name, \".csv\")) # Save results\n}\n\nresults <- list(\n  exp1 = read_csv(\"results_exp1.csv\"),\n  exp2 = read_csv(\"results_exp2.csv\")\n) |> bind_rows(.id = \"experiment\") \n\n```\n\n### Results\n\nLet's now evaluate this. As we saw in the slides, you can evaluate *spatially* or *textually*. Spatial evaluation consists of measuring the distance between the predicted and the actual location. Textual evaluation consists of checking if the predicted location matches the actual location.\n\n```{r evaluation_functions}\n# Data preprocessing and evaluation\nevaluation <- results |> \n  mutate(\n    chosen_option = if_else(chosen_option == 'Bristol', 'Bristol, City of', chosen_option),\n    result = case_when(\n      local_authority_district == chosen_option ~ \"Correct\",\n      local_authority_district != chosen_option ~ \"Incorrect\"\n    )\n  )\n\n# Create a lookup table for experiment descriptions\nexp_descriptions <- tibble(\n  experiment = names(experiments),\n  description = map_chr(experiments, ~paste(.x, collapse = \", \"))\n)\n\n# Classification accuracy table\nclassification_results <- evaluation |> \n  group_by(experiment, temperature) |>\n  summarise(\n    total = n(),\n    correct = sum(result == \"Correct\", na.rm = TRUE),\n    accuracy = round(correct / total, 3),\n    .groups = \"drop\"\n  ) |> \n  left_join(exp_descriptions, by = \"experiment\") |> \n  relocate(description, .after = experiment) |> \n  select(-experiment)\n\n# Display results\nprint(classification_results)\n```\n\n\n```{r}\n# Spatial evaluation with distance calculations\nspatial_results <- evaluation |>\n  left_join(\n    osm_with_districts_cands |> select(address, LAD24NM, latitude, longitude),\n    by = c('address', 'chosen_option' = 'LAD24NM')\n  ) |> \n  distinct(address, chosen_option, start, end, temperature, experiment, .keep_all = TRUE) |>\n  rename(true_long = Longitude, true_lat = Latitude, pred_long = longitude, pred_lat = latitude) |>\n  rowwise() |>\n  mutate(\n    distance = if_else(\n      is.na(pred_long) | is.na(pred_lat), \n      20039,  # Max distance for missing coordinates\n      geosphere::distHaversine(c(true_long, true_lat), c(pred_long, pred_lat)) / 1000\n    )\n  ) |>\n  ungroup()\n\n# Spatial metrics table\nspatial_metrics <- spatial_results |>\n  group_by(experiment, temperature) |>\n  summarise(\n    within_20km = round(mean(distance <= 20, na.rm = TRUE), 3),\n    within_161km = round(mean(distance <= 161, na.rm = TRUE), 3),\n    mean_distance = round(mean(distance, na.rm = TRUE), 1),\n    .groups = \"drop\"\n  )\n\n# Join with your spatial metrics\nspatial_metrics_labeled <- spatial_metrics |>\n  left_join(exp_descriptions, by = \"experiment\") |> \n  relocate(description, .after = experiment) |> \n  select(-experiment)\n\nprint(spatial_metrics_labeled)\n```\n\nOur experiments yielded mixed results across different experimental configurations. The classification accuracy varied significantly depending on the included contextual fields and temperature settings.\n\n#### Where to go from here?\nThese mixed results indicate substantial room for improvement through several approaches:\n\n*Model Selection:* A more sophisticated language model with better geographic knowledge and reasoning capabilities could significantly improve disambiguation accuracy.\n\n*Prompt Engineering:* More carefully crafted prompts that explicitly highlight geographic indicators and disambiguation strategies could enhance performance.\n\n*Enhanced Context:* Including or removing contextual information such as regional newspapers, local landmarks, or administrative hierarchies might improve results.\n\n*Thank you*\nAny questions or comments regarding this notebook, please let me know.\n**Contact:** s.bisiani\\@surrey.ac.uk\\\n**Code:** Available on [Github]()\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nWelcome to this live coding demonstration exploring **LLMs for Toponym Disambiguation** in UK local media. We will be using [LMUK-Geo](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SGVXIU), a benchmark dataset we annotated for the purpose of testing geoparsing methods on British local news articles. We'll do the following:\n\n-   toponym recognition using *spaCy* in Python, to extract locations\n\n-   candidate retrieval, to find real world candidates, using *OpenStreetMap Nominatim API*\n\n-   toponym resolution with *Ollama* and gemma2-9b model, to disambiguate\n\nAs such, you'll get an overview of the full geoparsing pipeline, and get a sense of popular frameworks and common challenges.\n\n#### Setup: Loading Libraries\n\nLet's start by loading our libraries:\n\n```{r setup}\n#| eval: false\n#| message: false\n#| warning: false\n\n# Load R libraries\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(reticulate)  # For Python integration\n  library(httr2)       # For API calls\n  library(jsonlite)    # For JSON handling\n  library(glue)        # For string templating\n  library(DT)          # For interactive tables\n  library(maps)\n})\n\n# use_virtualenv(\".venv/\")\nuse_virtualenv('.venv-mac/')\n# py_install(\"pandas\")\n# py_install('spacy[transformers]')\n# system(\"python -m spacy download en_core_web_sm\")\n```\n\n```{python libraries}\nimport pandas as pd\nimport numpy as np\nimport ast\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n------------------------------------------------------------------------\n\n## Dataset\n\nLet's examine our benchmark dataset:\n\n```{r gold-data-overview}\n#| message: false\n#| warning: false\n#| fig-cap: \"Distribution of locations inside the LMUK-Geo dataset\"\n#| column: margin\n\ngold_data = read_csv('data/LMUK-Geo.csv')\ngold_data |> glimpse()\n# gold_data |> select(doc) |> n_distinct()\n# gold_data |> select(domain) |> n_distinct()\n\nggplot() + \n  geom_polygon(data = map_data('world'), \n  aes(x = long, y = lat, group = group), \n  fill = 'gray90', \n  color = 'black') + \n  coord_fixed(xlim = c(-10,3), \n              ylim = c(50.3, 59))+\n  geom_point(data = gold_data, \n  aes(x = Longitude, y = Latitude, color = domain), \n  size = 2.5, alpha = 0.2) +\n  theme_minimal()+\n  theme(legend.position = \"none\")\n```\n\n------------------------------------------------------------------------\n\n## Toponym Recognition using spaCy\n\nRemember toponym recognition? The idea that, given some text, we extract the locations inside of it. Toponym recognition is a subset of Named Entity Recognition (NER), and, like NER, it's done in 3 ways: rules, learning, or hybrid. *Rules* consists of matching some patterns in the text. For example, anything matching the pattern 'City of' and the next word, or a word followed by 'Street', or 'Boulevard'.\n\n```{r NER-rule}\n# Sample text with locations\ntext <- \"I traveled from the City of Boston to New York, walking down Main Street and Broadway Boulevard. Then I visited Paris, France and returned to Los Angeles County.\"\n\n# Define simple patterns for location recognition\npatterns <- c(\n  \"City of \\\\w+\",           # City of [name]\n  \"\\\\w+ Street\",            # [name] Street  \n  \"\\\\w+ Boulevard\",         # [name] Boulevard\n  \"\\\\w+ County\",            # [name] County\n  \"\\\\w+, \\\\w+\"             # [City], [State/Country]\n)\n\n# Extract locations using pattern matching\nlocations <- c()\nfor(pattern in patterns) {\n  matches <- regmatches(text, gregexpr(pattern, text, perl = TRUE))[[1]]\n  locations <- c(locations, matches)\n}\n\n# Remove duplicates and display results\nunique_locations <- unique(locations)\nprint(unique_locations)\n```\n\nThe issue with rules is that to capture all instances you need a lot of them. The more rules you have the more false positives you will also have. Luckily, *learning-based* systems do a pretty good job nowadays. Sometimes researchers find it helpful to mix rules and learning-based methods, and thus this category is called *hybrid*. Learning-based NER trains machine learning models on large annotated datasets where humans have labeled entities like persons, locations, and organizations, allowing the system to automatically recognize similar patterns in new text without hand-crafted rules. [spaCy](https://spacy.io/usage) NER is a popular Python library that provides pre-trained neural network models for named entity recognition, offering out-of-the-box entity extraction in multiple languages with high accuracy and the ability to customize models for specific domains.\n\nThere are multiple models offered by spaCy, and which to choose mostly depends on where on the efficiency/accuracy spectrum you would like to be. In other words, how fast vs how correct. Sometimes, if your machine is not powerful enough, your choice might be driven by model size (guess: smaller = worse). You can check performances of various models by spaCy [on their website](images/clipboard-303220680.png)\\](https://spacy.io/usage/facts-figures)\n\n```{python spacy-NER-test}\nimport spacy\nfrom spacy import displacy\n\narticles = pd.read_csv('data/articles.csv')\nlen(articles)\narticles.head()\n\n# measure time\nnlp = spacy.load(\"en_core_web_trf\")  # Load the spaCy transformer model\n\nstart = pd.Timestamp.now()\ndoc = nlp(articles['article'][1])\nend = pd.Timestamp.now()\nprint(\"Time taken:\", end - start)\n\nentities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE']\nhtml = displacy.render(doc, style=\"ent\")\n\nnlp = spacy.load(\"en_core_web_sm\")  # Load the spaCy small model\n\nstart = pd.Timestamp.now()\ndoc = nlp(articles['article'][1])\nend = pd.Timestamp.now()\nprint(\"Time taken:\", end - start)\n\nentities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE']\nhtml1 = displacy.render(doc, style=\"ent\")\n```\n\nWe can display the results to get an intuitive understanding of what the NER model does.\n\n```{r}\n# Display the HTML from Python\nhtmltools::HTML(py$html)\nhtmltools::HTML(py$html1)\n```\n\nLet's now run these models on all our articles, and benchmark their performance against the gold annotation. The models look for three types of location entities:**GPE**: Geopolitical entities (countries, cities, states); **LOC**: Locations (mountains, water bodies, regions); **FAC**: Facilities (buildings, airports, highways). Each model's predictions are compred against the human-annotated *gold standard* using standard information retrieval metrics. We will define:\n\n-   **True Positives**: Geographic entities correctly identified by the model\\\n-   **False Positives**: Non-geographic terms incorrectly identified as locations\\\n-   **False Negatives**: Geographic entities that the model missed\\\n\nand use these to calculate:\n\n**Precision** = TP / (TP + FP)\\\n\\> *Of all the entities the model identified as locations, what percentage were actually correct?*\\\nHigh precision = low false alarm rate\n\n**Recall** = TP / (TP + FN)\\\n\\> *Of all the actual geographic entities in the text, what percentage did the model successfully find?*\\\nHigh recall = low miss rate\n\n**F1 Score** = 2 × (Precision × Recall) / (Precision + Recall)\\\n\\> Harmonic mean that balances precision and recall.\\\n\\> Useful single metric when you need both high precision AND high recall.\n\nWe will then produce a **comparison table** showing precision, recall, and F1 scores for both models, helping determine which performs better for toponym recognition tasks.\n\n```{python}\n# Load human-annotated gold standard data\ngold_df = pd.read_csv('data/gold_toponym_recognition.csv')\n\n# ------------------------------------------------------------\n# STEP 1: Parse the gold standard annotations\n# ------------------------------------------------------------\n# The entities_gold column contains string representations of dicts\n# We need to convert them to actual Python dictionaries\n\ndef safe_literal_eval(x):\n    \"\"\"Safely convert string to dictionary\"\"\"\n    if isinstance(x, dict):\n        return x\n    try:\n        return ast.literal_eval(x)\n    except (ValueError, SyntaxError):\n        return {}\n\ngold_df['entities_gold'] = gold_df['entities_gold'].apply(safe_literal_eval)\n\n# ------------------------------------------------------------\n# STEP 2: Extract entities using both spaCy models\n# ------------------------------------------------------------\nnlp_trf = spacy.load(\"en_core_web_trf\")  # Transformer (accurate but slow)\nnlp_sm = spacy.load(\"en_core_web_sm\")    # Small model (fast but less accurate)\n\n# We care about these location entity types:\n# GPE = Geopolitical entities (countries, cities, states)\n# LOC = Locations (mountains, water bodies, regions)  \n# FAC = Facilities (buildings, airports, highways)\nVALID_LABELS = {\"LOC\", \"GPE\", \"FAC\"}\n\ndef extract_entities(text, nlp_model):\n    \"\"\"Extract location entities from text using spaCy\"\"\"\n    doc = nlp_model(text)\n    return {\n        label: [ent.text.strip() for ent in doc.ents if ent.label_ == label]\n        for label in VALID_LABELS\n    }\n\n# Process all documents with both models\ntrf_results = []\nsm_results = []\n\nfor doc_text in tqdm(gold_df['doc'], desc=\"Processing documents\"):\n    trf_results.append(extract_entities(doc_text, nlp_trf))\n    sm_results.append(extract_entities(doc_text, nlp_sm))\n\n# Add model predictions to dataframes\ngold_df['entities_trf'] = trf_results\ngold_df['entities_sm'] = sm_results\n# gold_df.to_csv('data/gold_with_predictions.csv', index=False)\n# ------------------------------------------------------------\n# STEP 3: Calculate performance metrics\n# ------------------------------------------------------------\n\ndef evaluate_model(gold_entities, predicted_entities):\n    \"\"\"\n    Compare predicted entities against gold standard.\n    Returns precision, recall, and F1 scores.\n    \"\"\"\n    # Flatten all entity types into single lists and lowercase for fair comparison\n    gold_flat = []\n    pred_flat = []\n    \n    for entity_type in VALID_LABELS:\n        gold_flat.extend([e.lower() for e in gold_entities.get(entity_type, [])])\n        pred_flat.extend([e.lower() for e in predicted_entities.get(entity_type, [])])\n    \n    # Convert to sets for comparison\n    gold_set = set(gold_flat)\n    pred_set = set(pred_flat)\n    \n    # Calculate true positives, false positives, false negatives\n    tp = len(gold_set & pred_set)  # Correctly found\n    fp = len(pred_set - gold_set)  # Incorrectly found\n    fn = len(gold_set - pred_set)  # Missed\n    \n    return tp, fp, fn\n\n# Calculate metrics for each document\ntrf_metrics = []\nsm_metrics = []\n\nfor idx, row in gold_df.iterrows():\n    # Transformer model\n    tp, fp, fn = evaluate_model(row['entities_gold'], row['entities_trf'])\n    trf_metrics.append({'TP': tp, 'FP': fp, 'FN': fn})\n    \n    # Small model\n    tp, fp, fn = evaluate_model(row['entities_gold'], row['entities_sm'])\n    sm_metrics.append({'TP': tp, 'FP': fp, 'FN': fn})\n\n# ------------------------------------------------------------\n# STEP 4: Calculate overall performance \n# ------------------------------------------------------------\n\ndef calculate_micro_averaged_scores(metrics_list):\n    \"\"\"Calculate overall precision, recall, and F1 across all documents\"\"\"\n    total_tp = sum(m['TP'] for m in metrics_list)\n    total_fp = sum(m['FP'] for m in metrics_list)\n    total_fn = sum(m['FN'] for m in metrics_list)\n    \n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return precision, recall, f1\n\n# Get final scores\ntrf_precision, trf_recall, trf_f1 = calculate_micro_averaged_scores(trf_metrics)\nsm_precision, sm_recall, sm_f1 = calculate_micro_averaged_scores(sm_metrics)\n\nresults = pd.DataFrame({\n    \"Model\": [\"en_core_web_trf\", \"en_core_web_sm\"],\n    \"Precision\": [trf_precision, sm_precision],\n    \"Recall\": [trf_recall, sm_recall],\n    \"F1\": [trf_f1, sm_f1]\n})\n\nresults.round(3)\n```\n\nAs we can see, the transformer model is highly superior (althought slower) to the small model. Just so you know, these represent the extremes of the spaCy spectrum and there are other models you could try.  Let's now save the toponyms extracted by our best model, to use in the next sections.\n\n```{python}\nlong_data = []\nfor idx, row in gold_df.iterrows():\n    for entity_type, entities in row['entities_trf'].items():\n        for entity in entities:\n            long_data.append({'doc_id': idx, 'doc': row['doc'], 'entity': entity})\n\npd.DataFrame(long_data).to_csv('data/entities_long.csv', index=False)\n```\n\n------------------------------------------------------------------------\n\n## Candidate Retrieval\n\nFor candidate retrieval, we will use [OpenStreetMap's Nominatim](https://nominatim.org/), an API service that fetches data from OSM's geographic knowledge database. We will use the tidygeocoder package in R, which provides a simple and consistent interface for geocoding and reverse geocoding using various services, including OSM Nominatim. It allows you to convert addresses or place names into geographic coordinates (latitude and longitude) and vice versa.\n\nJust so you know, another useful resource (for UK locations) is Ordnance Survey Open Names, a comprehensive database of place names, roads numbers and postcodes for Great Britain (but no Northern Ireland unfortunately): <https://www.ordnancesurvey.co.uk/products/os-open-names>. Downloading OS Open Names is very simple. From the platform you just click on your desired format and get the data for free. It's updated quarterly.\n\n    ![](images/clipboard-3609027799.png){width=\"358\"}\n\n```{r}\n# Load data\ngold_df <- read_csv('data/entities_long.csv')\nunique_entities <- gold_df |>\n  select(entity) |>\n  distinct()\n# write_csv(unique_entities, 'data/unique_entities.csv')\n\n# OSM candidates (this will take around 8minutes for 470 entities)\nlibrary(tidygeocoder)\n# unique_entities <- read_csv('data/unique_entities.csv')\nosm_candidates <- unique_entities |> \n  geocode(entity, method = 'osm', lat = latitude , long = longitude, limit = 10, custom_query = list(countrycodes = \"gb\"), return_input = FALSE)\nwrite_csv(osm_candidates, 'data/osm_candidates.csv')\n```\n\nBefore we move on to disambiguation let's calculate which district each coordinate falls into. This will help us later on in the disambiguation process.\n\n```{r}\nosm_candidates <- read_csv('data/osm_candidates.csv')\nlibrary(sf)\n# Load UK districts shapefile\nuk_districts <- st_read(\"lad_boundaries/LAD_MAY_2024_UK_BFE.shp\")\nuk_districts <- st_transform(uk_districts, crs = 4326)  # Ensure it's in WGS84\n\n# Convert OSM candidates to sf object\nosm_sf <- st_as_sf(osm_candidates |> drop_na(), coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Perform spatial join to find which district each point falls into\nosm_with_districts <- st_join(osm_sf, uk_districts, join = st_within)\nosm_with_districts_cands <- osm_with_districts |> \n  filter(!is.na(LAD24NM)) |> \n  mutate(\n    longitude = st_coordinates(geometry)[,1],\n    latitude  = st_coordinates(geometry)[,2]\n  )\n\nwrite_csv(osm_with_districts_cands |> st_drop_geometry(), 'data/osm_candidates_with_districts.csv')\n```\n\n------------------------------------------------------------------------\n\n## Toponym Resolution\n\n### Ollama\nTo use ollama in R, you can directly communicate with the local server using the `httr2` package, or you can use the `ellmer`, '[ollamar](https://hauselin.github.io/ollama-r/)' or 'rollama' packages which provide a convenient interface. Regardless, your first need install Ollama on your machine.\n\nIf you choose the 'httr2' route, you can refer to the [Ollama API documentation](https://ollama.com/docs/api) for details on how to structure your requests. I also recommend this [blog post](https://tomsing1.github.io/blog/posts/vectorsearch/).\n\n```{r}\n# this is ellmer works\nchat <- ellmer::chat_ollama(\"Be terse\", model = \"gemma3:1b\", echo = \"none\")\nchat$chat(\"who is Super Mario's best friend?\")\n\n# it's longer code, but more robust over time with httr2\nhttr2::request(\"http://localhost:11434\") |> \n  httr2::req_url_path(\"/api/generate\") |>\n  httr2::req_headers(\"Content-Type\" = \"application/json\") |>\n  httr2::req_body_json(\n    list(\n      model = \"gemma3:1b\",\n      prompt = \"Who is Super Mario's best friend?\",\n      stream = FALSE,\n      options = list(seed = 123)  # reproducible seed\n    )\n  ) |> \n  httr2::req_perform() |> httr2::resp_body_json() |>\n    getElement(\"response\")\n\n# a useful wrapper is ollamar, which hides this code behind a function\nollamar::generate(\"gemma3:1b\", \"Who is Super Mario's best friend?\", output = 'text')\n```\n\n### Experiment Setup\nLet's try setup an experiment where we query gemma3-1b to disambiguate a toponym. We will provide it with the context of the article where the toponym was found, and a list of candidates retrieved from the gazetteers. The model will then select the most likely candidate.\n\n```{r load-data}\n# merge back candidates with articles and entities\ngold_data <- read_csv('data/LMUK-Geo.csv')\nosm_with_districts_cands <- read_csv('data/osm_candidates_with_districts.csv')\n\n# now let's select a small sample of 10 articles to work with\nset.seed(42)\nsample_docs <- gold_data |> \n  distinct(doc) |>            # make sure articles are unique\n  slice_sample(n = 10) |> \n  pull(doc)                   # vector of 10 article texts\n\n# Step 2: keep all toponyms + candidates for those articles\nsample <- osm_with_districts_cands |> \n  sf::st_drop_geometry() |>\n  group_by(address) |>\n  summarise(candidates = str_c(unique(LAD24NM), collapse = \"; \"), .groups = \"drop\") |>\n  left_join(gold_data, by = c(\"address\" = \"text\")) |> \n  filter(doc %in% sample_docs)\n  \n# Define temperatures to test\ntemperatures <- as.character(c(0,1))\ndata_expanded <- expand_grid(\n  temperature = temperatures, \n  sample\n) |> \n  mutate(model = 'gemma3:1b')\n```\n\n\n```{r experiments-setup}\n# Define experiments: which fields to include\nexperiments <- list(\n  exp1 = c(\"Outlet coverage LAD\", \"Domain\"),\n  exp2 = c(\"Domain\")  # Only Domain\n)\n\n# Define prompt types\nprompt <- \"The task is mapping an entity (a toponym) to the Local Authority District (LAD) in which it is situated. Your goal is to select the correct option from the list provided. Instructions:\n1. Review Entity and Article:\n- Identify the toponym (location name).\n- Read the article carefully to understand the context.\nExample:\nEntity: King's Head pub.\nArticle: Incident outside the King's Head pub on Main Street, Guildford.\nUse surrounding text to infer the location (e.g., Guildford).\n\n2. Check Metadata where provided:\n- Domain: The publisher’s domain may provide geographic context.\n- Outlet coverage LAD: The Local Authority District covered by the outlet which published the article.\n- Other Entities: Other entities present in the same articles and their candidates.\n\n3. Select answer from options:\nChoose the correct answer from the options based on context.\nOptions:\n- A list of applicable Districts, if any.\n- “LAD not in options” (choose if correct District is missing).\n- “Entity is not a location” if applicable.\n- “Entity is outside the UK” for non-UK locations.\n- “Entity spans across several districts (e.g., a region)“ for entities that are not specific to a single LAD (e.g., Wales, Sussex).\n- “Unsure” if uncertain.\n\n4. Generate Response:\n- Format your response as JSON:\n{\n  \\'chosen_option\\': \\'Your choice\\',\n  \\'reasoning\\': \\'Your reasoning\\'\n}\"\n\nclassification_question <- \"Which of the options provided best represents the Local Authority District (LAD) for the entity provided, based on the context in the article? Ensure the response is strictly in JSON format with no additional text, explanations, or commentary outside of the JSON object. Match the JSON schema indicated. Example of output: {\\'chosen_option\\': \\'Fife\\', \\'reasoning\\': \\'The article refers to a toponym situated in Fife.\\'}\"\n```\n\n```{r main-function}\nlibrary(jsonlite)\ndetach(package:maps, unload=TRUE)\n\nquery_llm <- function(data, system_message, classification_question, included_fields = c(\"Outlet coverage LAD\", \"Domain\")) {\n  # Create prompts for each row\n  prompts <- data_expanded %>%\n    rowwise() |> \n    mutate(\n      test_prompt = glue(\n        \"{prompt}\\n\",\n        \"Entity: {address}\\n\\n\",\n        \"Article: {doc}\\n\\n\",\n        if (\"Outlet coverage LAD\" %in% included_fields) {\n          \"Outlet coverage LAD: {domain_lad}\\n\"\n        } else {\n          \"\"\n        },\n        if (\"Domain\" %in% included_fields) {\n          \"Domain: {domain}\\n\"\n        } else {\n          \"\"\n        },\n        \"Options: {candidates}\\n\",  # Fix: use candidates instead of options_str\n        \"{classification_question}\"\n      )\n    ) %>%\n    pull(test_prompt)\n  \n  # Create requests\n  reqs <- map2(prompts, data_expanded$temperature, function(prompt, temp) {  \n    httr2::request(\"http://localhost:11434\") %>%\n      httr2::req_url_path(\"/api/generate\") %>%\n      httr2::req_headers(\"Content-Type\" = \"application/json\") %>%\n      httr2::req_body_json(list(\n        model = \"gemma3:1b\", \n        prompt = prompt,\n        stream = FALSE,\n        format = \"json\",\n        keep_alive = \"10s\",\n        options = list(seed = 42, temperature = as.numeric(temp))  # Use temp parameter\n      ))\n  })\n  \n  # Make parallel requests\n  resps <- httr2::req_perform_parallel(reqs, on_error = \"continue\", progress = TRUE)\n  \n  # Process results\n  results <- purrr::map(resps, function(resp) {  # Use purrr::map explicitly\n    # Step 1: parse outer JSON\n    outer <- httr2::resp_body_json(resp)\n    # Step 2: parse inner 'response' JSON safely\n    parsed_inner <- tryCatch({\n      fromJSON(outer$response)\n    }, error = function(e) NULL)\n    tibble(\n      chosen_option = if (!is.null(parsed_inner)) parsed_inner$chosen_option else NA,\n      reasoning     = if (!is.null(parsed_inner)) parsed_inner$reasoning else NA\n    )\n  })\n  \n  # Combine into a single data frame\n  results_df <- bind_rows(results)\n  \n  # Bind with original data to preserve address, doc, and other fields\n  final_result <- bind_cols(\n    data %>% select(address, doc, everything()),  # Keep all original columns\n    results_df\n  )\n  \n  return(final_result)  # Don't forget to return the result!\n}\n\n# Simple iteration through experiments\nfor (exp_name in names(experiments)) {\n  included_fields <- experiments[[exp_name]]\n  \n  # Run the query\n  result <- query_llm(\n    data = data_expanded,\n    system_message = prompt,\n    classification_question = classification_question,\n    included_fields = included_fields\n  )\n  write_csv(result, paste0(\"results_\", exp_name, \".csv\")) # Save results\n}\n\nresults <- list(\n  exp1 = read_csv(\"results_exp1.csv\"),\n  exp2 = read_csv(\"results_exp2.csv\")\n) |> bind_rows(.id = \"experiment\") \n\n```\n\n### Results\n\nLet's now evaluate this. As we saw in the slides, you can evaluate *spatially* or *textually*. Spatial evaluation consists of measuring the distance between the predicted and the actual location. Textual evaluation consists of checking if the predicted location matches the actual location.\n\n```{r evaluation_functions}\n# Data preprocessing and evaluation\nevaluation <- results |> \n  mutate(\n    chosen_option = if_else(chosen_option == 'Bristol', 'Bristol, City of', chosen_option),\n    result = case_when(\n      local_authority_district == chosen_option ~ \"Correct\",\n      local_authority_district != chosen_option ~ \"Incorrect\"\n    )\n  )\n\n# Create a lookup table for experiment descriptions\nexp_descriptions <- tibble(\n  experiment = names(experiments),\n  description = map_chr(experiments, ~paste(.x, collapse = \", \"))\n)\n\n# Classification accuracy table\nclassification_results <- evaluation |> \n  group_by(experiment, temperature) |>\n  summarise(\n    total = n(),\n    correct = sum(result == \"Correct\", na.rm = TRUE),\n    accuracy = round(correct / total, 3),\n    .groups = \"drop\"\n  ) |> \n  left_join(exp_descriptions, by = \"experiment\") |> \n  relocate(description, .after = experiment) |> \n  select(-experiment)\n\n# Display results\nprint(classification_results)\n```\n\n\n```{r}\n# Spatial evaluation with distance calculations\nspatial_results <- evaluation |>\n  left_join(\n    osm_with_districts_cands |> select(address, LAD24NM, latitude, longitude),\n    by = c('address', 'chosen_option' = 'LAD24NM')\n  ) |> \n  distinct(address, chosen_option, start, end, temperature, experiment, .keep_all = TRUE) |>\n  rename(true_long = Longitude, true_lat = Latitude, pred_long = longitude, pred_lat = latitude) |>\n  rowwise() |>\n  mutate(\n    distance = if_else(\n      is.na(pred_long) | is.na(pred_lat), \n      20039,  # Max distance for missing coordinates\n      geosphere::distHaversine(c(true_long, true_lat), c(pred_long, pred_lat)) / 1000\n    )\n  ) |>\n  ungroup()\n\n# Spatial metrics table\nspatial_metrics <- spatial_results |>\n  group_by(experiment, temperature) |>\n  summarise(\n    within_20km = round(mean(distance <= 20, na.rm = TRUE), 3),\n    within_161km = round(mean(distance <= 161, na.rm = TRUE), 3),\n    mean_distance = round(mean(distance, na.rm = TRUE), 1),\n    .groups = \"drop\"\n  )\n\n# Join with your spatial metrics\nspatial_metrics_labeled <- spatial_metrics |>\n  left_join(exp_descriptions, by = \"experiment\") |> \n  relocate(description, .after = experiment) |> \n  select(-experiment)\n\nprint(spatial_metrics_labeled)\n```\n\nOur experiments yielded mixed results across different experimental configurations. The classification accuracy varied significantly depending on the included contextual fields and temperature settings.\n\n#### Where to go from here?\nThese mixed results indicate substantial room for improvement through several approaches:\n\n*Model Selection:* A more sophisticated language model with better geographic knowledge and reasoning capabilities could significantly improve disambiguation accuracy.\n\n*Prompt Engineering:* More carefully crafted prompts that explicitly highlight geographic indicators and disambiguation strategies could enhance performance.\n\n*Enhanced Context:* Including or removing contextual information such as regional newspapers, local landmarks, or administrative hierarchies might improve results.\n\n*Thank you*\nAny questions or comments regarding this notebook, please let me know.\n**Contact:** s.bisiani\\@surrey.ac.uk\\\n**Code:** Available on [Github]()\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"live-coding-demo.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":"cosmo","title":"Location Extraction from News Articles, using R and Python","author":"Simona Bisiani","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}